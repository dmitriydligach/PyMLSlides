\documentclass{beamer}
\usepackage{latexsym}
\usepackage{graphicx}
\usetheme{Warsaw}

\title{Chapter 7}
\subtitle{Ensemble Classifiers}

\begin{document}
\maketitle

\begin{frame}
  \frametitle{Learning with ensembles}
  \begin{itemize}
  \item Our goal is to combined multiple classifiers
  \item Mixture of experts, e.g. 10 experts
  \item Predictions more accurate and robust
  \item Provide an intuition why this might work
  \item Simplest approach: majority voting
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Majority voting}
  \begin{itemize}
  \item Majority voting refers to binary setting
  \item Can easily generalize to multi-class: plurality voting
  \item Select class label that receives the most votes (mode)
  \end{itemize}
  \vspace{0.2in}
  \center
  \includegraphics[scale=0.4]{Code/ch07/images/07_01.png}
\end{frame}

\begin{frame}
  \frametitle{Combining predictions: options}
  \begin{itemize}
  \item Train $m$ classifiers $C_1,\dots,C_m$
  \item Build ensemble using different classification algorithms (e.g. SVM, logistic regression, etc.)
  \item Use the same algorithm but fit different subsets of the training set (e.g. random forest)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{General approach}
  \center
  \includegraphics[scale=0.35]{Code/ch07/images/07_02.png}
\end{frame}

\begin{frame}
  \frametitle{Combining predictions via majority voting}
  We have predictions of individual classifiers $C_j$ and need to select the final class label $\hat{y}$
  \[
  \hat{y} = mode \{ C_1 (\mathbf{x}), C_2 (\mathbf{x}), \dots, C_m (\mathbf{x}) \}
  \]
  For example, in a binary classification task where $class_1 = -1$ and $class_2 = +1$, we can write the majority vote prediction as follows:
  \[
  C(\mathbf{x}) = sign \Bigg[ \sum_{j}^{m} C_j (\mathbf{x}) \Bigg] = \begin{cases}
        1 & \text{ if } \sum_j C_j (\mathbf{x}) \ge 0 \\
        -1 & \text{ otherwise }
     \end{cases}
  \]
\end{frame}

\begin{frame}
  \frametitle{Intuition why ensembles can work better}
  Assume that all $n$ base classifiers have the same error rate $\epsilon$. We can expresss the probability of an error of an ensemble can be expressed as a probability mass function of a binomial distribution:
  \[
  P(y \ge k) = \sum_{k}^{n} \binom{n}{k} \epsilon^k (1 - \epsilon)^{n-k} = \epsilon_{\text{ensemble}}
  \]
  Here, $\binom{n}{k}$ is the binomial coefficient \textit{n choose k}. In other words, we compute the probability that the prediction of the ensemble is wrong.
\end{frame}

\begin{frame}
  \frametitle{Example}
  Imagine we have 11 base classifiers ($n=11$) with an error rate of 0.25 ($\epsilon = 0.25$):
  \[
  P(y \ge k) = \sum_{k=6}^{11} \binom{11}{k} 0.25^k (1 - 0.25)^{11-k} = 0.034
  \]
  So the error rate of the ensemble of $n=11$ classifiers is much lower than the error rate of the individual classifiers.
\end{frame}

\begin{frame}
  \frametitle{Same reasoning applied to a wider range of error rates}
  \center
  \includegraphics[scale=0.6]{Code/ch07/images/07_03.png}
\end{frame}

\begin{frame}
  \frametitle{Boostrap aggregation (bagging)}
  \begin{itemize}
  \item We used the entire training set for the majority vote classifier
  \item Here we draw \textbf{bootstrap samples}
  \item In statistics, \textbf{bootstrapping} is any test or metric that relies on \textbf{random sampling with replacement}.
  \item Hypothesis testing: bootstrapping often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt
  \item The basic idea of bootstrapping is that inference about a population from sample data, can be modelled by resampling with replacement the sample data and performing inference about a sample from resampled data.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bagging}
  \center
  \includegraphics[scale=0.08]{Code/ch07/images/07_06.png}
\end{frame}

\begin{frame}
  \frametitle{Boostrapping example}
  \center
  \includegraphics[scale=0.24]{Code/ch07/images/07_07.png}
  \begin{itemize}
  \item Seven training examples
  \item Sample randomly with replacement
  \item Use each boostrap sample to train a classifier $C_j$
  \item $C_j$ is typically a decision tree
  \item \textbf{Random Forests}: also use random feature subsets
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Bagging in scikit-learn}
  \begin{itemize}
  \item Instantiate a decision tree classifier
  \item Make a bagging classifier with decision trees
  \item Check that the accuracy is higher for the bagging classifier
  \item \href{https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch07/ch07.ipynb}{\beamergotobutton{PML github}}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
  \begin{itemize}
  \item
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
  \begin{itemize}
  \item
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
  \begin{itemize}
  \item
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
  \begin{itemize}
  \item
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
  \begin{itemize}
  \item
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
  \begin{itemize}
  \item
  \end{itemize}
\end{frame}

\end{document}
